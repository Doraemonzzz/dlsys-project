{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22354,"status":"ok","timestamp":1673323506500,"user":{"displayName":"cat blue","userId":"12317392281162635222"},"user_tz":-480},"id":"7b5u0n2SPK_8","outputId":"c666d79c-586e-4e57-b7f7-4115187c2b06"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/10714/project\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pybind11\n","  Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.4/222.4 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pybind11\n","Successfully installed pybind11-2.10.3\n"]}],"source":["# Code to set up the project\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/10714\n","!git clone https://github.com/Doraemonzzz/dlsys-project.git\n","%cd dlsys-project\n","\n","!pip3 install pybind11"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":328,"status":"ok","timestamp":1673323510052,"user":{"displayName":"cat blue","userId":"12317392281162635222"},"user_tz":-480},"id":"E0cAIgEtPhu-"},"outputs":[],"source":["import sys\n","sys.path.append('./python')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1673323512883,"user":{"displayName":"cat blue","userId":"12317392281162635222"},"user_tz":-480},"id":"1JiJvc2OPkIa"},"outputs":[],"source":["# Download the datasets you will for this project\n","import urllib.request\n","import os\n","\n","# Download CIFAR-10 dataset\n","if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n","    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n","    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28601,"status":"ok","timestamp":1673323709677,"user":{"displayName":"cat blue","userId":"12317392281162635222"},"user_tz":-480},"id":"ix9Hijfe2Y8Z","outputId":"da432032-1e11-40f7-fc33-7cf0d34e2d48"},"outputs":[{"name":"stdout","output_type":"stream","text":["rm -rf build python/needle/backend_ndarray/ndarray_backend*.so\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Found Python: /usr/bin/python3.8 (found version \"3.8.16\") found components: Development Interpreter Development.Module Development.Embed \n","-- Performing Test HAS_FLTO\n","-- Performing Test HAS_FLTO - Success\n","-- Found pybind11: /usr/local/lib/python3.8/dist-packages/pybind11/include (found version \"2.10.3\")\n","-- Looking for pthread.h\n","-- Looking for pthread.h - found\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n","-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed\n","-- Looking for pthread_create in pthreads\n","-- Looking for pthread_create in pthreads - not found\n","-- Looking for pthread_create in pthread\n","-- Looking for pthread_create in pthread - found\n","-- Found Threads: TRUE  \n","-- Found CUDA: /usr/local/cuda (found version \"11.2\") \n","-- Found cuda, building cuda backend\n","Tue Jan 10 04:08:55 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   56C    P0    28W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","-- Autodetected CUDA architecture(s):  7.5\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/drive/MyDrive/10714/project/build\n","make[1]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","make[2]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","[-25%] \u001b[32mBuilding CXX object CMakeFiles/ndarray_backend_cpu.dir/src/ndarray_backend_cpu.cc.o\u001b[0m\n","[  0%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cpu.cpython-38-x86_64-linux-gnu.so\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n","[  0%] Built target ndarray_backend_cpu\n","make[3]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n","make[3]: Entering directory '/content/drive/MyDrive/10714/project/build'\n","[ 50%] \u001b[32m\u001b[1mLinking CXX shared module ../python/needle/backend_ndarray/ndarray_backend_cuda.cpython-38-x86_64-linux-gnu.so\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n","[ 50%] Built target ndarray_backend_cuda\n","make[2]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n","make[1]: Leaving directory '/content/drive/MyDrive/10714/project/build'\n"]}],"source":["# complie code\n","!make clean\n","!make"]},{"cell_type":"markdown","metadata":{"id":"bd8YXT-ZrAkk"},"source":["# Batch matrix multiplication\n","In the course, we learned about the implementation of matrix multiplication, which is a mapping of the form:\n","$$\n","m\\times n, n\\times p \\to m\\times p.\n","$$\n","Due to subsequent needs, we implement the batch version of matrix multiplication here, namely `bmm`:\n","$$\n","b\\times m\\times n,b\\times  n\\times p \\to b\\times m\\times p.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"CyStjKRow11O"},"source":["## Implementation\n","Cpu version:\n","```c++\n","void BatchMatmul(const AlignedArray& a, const AlignedArray& b, AlignedArray* out, uint32_t b_, uint32_t m, uint32_t n,\n","            uint32_t p) {\n","  for (int l = 0; l < b_; l++) {\n","    for (int i = 0; i < m; i++) {\n","      for (int j = 0; j < p; j++) {\n","        float res = 0;\n","        for (int k = 0; k < n; k++) {\n","          res += a.ptr[l * m * n + i * n + k] * b.ptr[l * n * p + k * p + j];\n","        }\n","        out->ptr[l * m * p + i * p + j] = res;\n","      }\n","    }\n","  }\n","}\n","```\n","\n","Cuda Version:\n","```cpp\n","// batch matrix multiply\n","__global__ void BatchMatmulKernel(const scalar_t* a, const scalar_t* b, scalar_t* out, uint32_t B, uint32_t M, uint32_t N,\n","            uint32_t P) {\n","  size_t x = blockIdx.x * blockDim.x + threadIdx.x;\n","  size_t y = blockIdx.y * blockDim.y + threadIdx.y;\n","  if (x < M && y < P) {\n","    for (int l = 0; l < B; l++) {\n","      float res = 0;\n","      for (int z = 0; z < N; z++) {\n","        res += a[l * M * N + x * N + z] * b[l * N * P + z * P + y];\n","      }\n","      out[l * M * P + x * P + y] = res;\n","    }\n","  }\n","}\n","\n","void BatchMatmul(const CudaArray& a, const CudaArray& b, CudaArray* out, uint32_t B, uint32_t M, uint32_t N,\n","            uint32_t P) {\n","  /**\n","   * Multiply two (compact) matrices into an output (also comapct) matrix.  You will want to look\n","   * at the lecture and notes on GPU-based linear algebra to see how to do this.  Since ultimately\n","   * mugrade is just evaluating correctness, you _can_ implement a version that simply parallelizes\n","   * over (i,j) entries in the output array.  However, to really get the full benefit of this\n","   * problem, we would encourage you to use cooperative fetching, shared memory register tiling, \n","   * and other ideas covered in the class notes.  Note that unlike the tiled matmul function in\n","   * the CPU backend, here you should implement a single function that works across all size\n","   * matrices, whether or not they are a multiple of a tile size.  As with previous CUDA\n","   * implementations, this function here will largely just set up the kernel call, and you should\n","   * implement the logic in a separate MatmulKernel() call.\n","   * \n","   *\n","   * Args:\n","   *   a: compact 2D array of size b x m x n\n","   *   b: comapct 2D array of size b x n x p\n","   *   out: compact 2D array of size b x m x p to write the output to\n","   *   M: rows of a / out\n","   *   N: columns of a / rows of b\n","   *   P: columns of b / out\n","   */\n","\n","  /// BEGIN YOUR SOLUTION\n","  dim3 DimGrid((M / d) + 1, (P / d) + 1, 1);\n","  dim3 DimBlock(d, d, 1);\n","\n","  BatchMatmulKernel<<<DimGrid, DimBlock>>>(a.ptr, b.ptr, out->ptr, B, M, N, P);\n","  /// END YOUR SOLUTION\n","}\n","\n","```\n","The code can be found in `project/src`. \n","To support `bmm`, we should also change the `__matmul__` in `./python/needle/backend_ndarray/ndarray.py`:\n","```cpp\n","    ### Matrix multiplication\n","    def __matmul__(self, other):\n","        \"\"\"Matrix multplication of two arrays.  This requires that both arrays\n","        be 2D (i.e., we don't handle batch matrix multiplication), and that the\n","        sizes match up properly for matrix multiplication.\n","\n","        In the case of the CPU backend, you will implement an efficient \"tiled\"\n","        version of matrix multiplication for the case when all dimensions of\n","        the array are divisible by self.device.__tile_size__.  In this case,\n","        the code below will restride and compact the matrix into tiled form,\n","        and then pass to the relevant CPU backend.  For the CPU version we will\n","        just fall back to the naive CPU implementation if the array shape is not\n","        a multiple of the tile size\n","\n","        The GPU (and numpy) versions don't have any tiled version (or rather,\n","        the GPU version will just work natively by tiling any input size).\n","        \"\"\"\n","        assert (self.ndim == 2 and other.ndim == 2) or (self.ndim == 3 and other.ndim == 3)\n","        assert self.shape[-1] == other.shape[-2]\n","\n","        if self.ndim == 2:\n","            m, n, p = self.shape[0], self.shape[1], other.shape[1]\n","            out = NDArray.make((m, p), device=self.device)\n","            self.device.matmul(\n","                self.compact()._handle, other.compact()._handle, out._handle, m, n, p\n","            )\n","            return out\n","        else:\n","            b, m, n, p = self.shape[0], self.shape[1], self.shape[2], other.shape[-1]\n","            out = NDArray.make((b, m, p), device=self.device)\n","            self.device.batchmatmul(\n","                self.compact()._handle, other.compact()._handle, out._handle, b, m, n, p\n","            )\n","            return out\n","```"]},{"cell_type":"markdown","metadata":{"id":"-5h6CK8EWK4x"},"source":["# Linear Attention\n","In the course, we learned **Attention**. Although the performance is excellent, the time complexity of Attention is $O(n^2d)$, where $n$ is the sequence length and $d$ is the feature dimension. To alleviate this, we can use **Linear Attention**.\n","Linear Attention is calculated as follows:\n","$$\n","\\mathbf O = \\mathrm{diag}\\{\\phi(\\mathbf Q) \\phi (\\mathbf K^{\\top}) \\mathbf 1_n \\}^{-1} \\phi(\\mathbf Q) \\phi (\\mathbf K^{\\top}) \\mathbf V, \\tag 1\n","$$\n","where $\\mathbf Q, \\mathbf K, \\mathbf V \\in \\mathbb R^{n\\times d}$. Note that in this naive implementation, the time complexity is still $O(n^2 d), we will improve this later.$\n","\n","The difference between **Linear Attention** and **Attention** is as follows:\n","- Added feature map $\\phi$;\n","- No exp activation function is used for $\\phi(\\mathbf Q) \\phi (\\mathbf K^{\\top}$ before normalization;\n","- The associative law of matrix multiplication can be used for efficient calculation,\n","\n","For the third point, note that for matrixs $\\mathbf A, \\mathbf B, \\mathbf C$, the following equation holds:\n","$$\n","\\mathbf A \\mathbf B \\mathbf C = \\mathbf A (\\mathbf B \\mathbf C ).\n","$$\n","This is known as the associative law of matrix multiplication. Using this property, Equation (1) is equivlant to:\n","$$\n"," \\mathrm{diag}\\{\\phi(\\mathbf Q) [\\phi (\\mathbf K^{\\top}) \\mathbf 1_n] \\}^{-1} [\\phi(\\mathbf Q)[\\phi (\\mathbf K^{\\top}) \\mathbf V]]. \\tag 2\n","$$\n","Let's analyze the computational complexity of the above formula:\n","- $\\mathrm{diag}\\{(\\phi(\\mathbf Q) \\phi (\\mathbf K^{\\top}) \\mathbf 1_n \\}^{-1}$\n","  - $ \\phi (\\mathbf K^{\\top}) \\mathbf 1_d: (d, n), (n, 1)\\to (d, 1), O(nd);$\n","  - $\\phi(\\mathbf Q) [\\phi (\\mathbf K^{\\top}) \\mathbf 1_n]: (n, d), (d, 1) \\to (n, 1), O(nd)$;\n","  - $ \\mathrm{diag}\\{\\phi(\\mathbf Q) \\phi (\\mathbf K^{\\top}) \\mathbf 1_n \\}^{-1} :(n, 1) \\to (n, n), O(n)$l\n","- $\\phi(\\mathbf Q)[\\phi (\\mathbf K^{\\top}) \\mathbf V]$:\n","  - $\\phi (\\mathbf K^{\\top}) \\mathbf V: (d, n), (n, d) \\to (d, d), O(nd^2)$;\n","  - $\\phi(\\mathbf Q)[\\phi (\\mathbf K^{\\top}) \\mathbf V]:(n, d), (d, d) \\to (n, d), O(nd^2)$,\n","\n","So the total time complexity is $O(nd^2)$, when $n\\gg d$, $n d^2 \\ll n^2 d$, which results much faster computation.\n"]},{"cell_type":"markdown","metadata":{"id":"jWpbtqm0nWiI"},"source":["## Implementation\n","In this part, we give the implementation of `LinearAttention`, here we use ReLU as $\\phi$, `eps` is added to prevent the value of the denominator from underflowing. To support 3D linear projection, we implemented `Linear3D`. The code is also available at `./python/needle/linear_transformer.py`:\n","```python\n","# 3D version of LinearLayer\n","class Linear3D(Module):\n","    def __init__(self, in_features, out_features, device=None, dtype=\"float32\"):\n","        super().__init__()\n","        self.linear = Linear(in_features, out_features, device=device, dtype=dtype)\n","        \n","    def forward(self, x):\n","        b, n, d = x.shape\n","        # b, n, d -> b * n, d\n","        x = ops.reshape(x, (b * n, d))\n","        # b * n, d -> b * n, e\n","        x = self.linear(x)\n","        # b * n, e -> b, n, e\n","        d = x.shape[-1]\n","        x = ops.reshape(x, (b, n, d))\n","        \n","        return x\n","\n","class LinearAttention(Module):\n","    def __init__(self, d, h, device=None, dtype=\"float32\"):\n","        super().__init__()\n","        self.qkv = Linear3D(d, 3 * d, device=device, dtype=dtype)\n","        self.out = Linear3D(d, d, device=device, dtype=dtype)\n","        self.d = d\n","        self.h = h\n","        self.e = self.d // self.h\n","        self.act = ReLU()\n","    \n","    def forward(self, x, eps=1e-5):\n","        # b, n, d -> b, n, 3 * d\n","        qkv = self.qkv(x)\n","        # get shape\n","        b = qkv.shape[0]\n","        n = qkv.shape[1]\n","        # reshape\n","        qkv = ops.reshape(qkv, (b, n, 3, self.d))\n","        # split\n","        q, k, v = ops.split(qkv, axis=2)\n","        # b, n, d -> b, n, h, e\n","        q, k, v = [ops.reshape(x, (b, n, self.h, self.e)) for x in (q, k, v)]\n","        # b, n, h, e -> b, h, n, e\n","        q, k, v = [ops.transpose(x, (2, 1)) for x in (q, k, v)]\n","        # b, h, n, e -> b * h, n, e\n","        q, k, v = [ops.reshape(x, (b * self.h, n, self.e)) for x in (q, k, v)]\n","        # act\n","        q = self.act(q) + eps\n","        k = self.act(k) + eps\n","        # (b * h, n, e), (b * h, n, e) -> (b * h, e, e)\n","        kv = ops.matmul(ops.transpose(k, (2, 1)), v)\n","        # (b * h, n, e), (b * h, e, e) -> (b * h, n, e)\n","        output = ops.matmul(q, kv)\n","        # qk denom\n","        # 1, n, 1 -> b * h, n, 1\n","        ones = init.ones(n, device=output.device, dtype=output.dtype)\n","        ones = ops.broadcast_to(ops.reshape(ones, (1, n, 1)), (k.shape[0], n, 1))\n","        # (b * h, n, e), (b * h, n, 1) -> (b * h, e, 1)\n","        t1 = ops.matmul(ops.transpose(k, (2, 1)), ones)\n","        # (b * h, n, e), (b * h, e, 1) -> (b * h, n, 1)\n","        t2 = ops.matmul(q, t1)\n","        # (b * h, n, e), (b * h, n, 1) -> (b * h, n, e)\n","        output = ops.divide(output, ops.broadcast_to(t2, output.shape))\n","        # (b * h, n, e) -> (b, h, n, e)\n","        output = ops.reshape(output, (b, self.h, n, self.e))\n","        # (b, h, n, e) -> (b, n, h, e)\n","        output = ops.transpose(output, (2, 1))\n","        # (b, n, h, e) -> (b, n, d)\n","        output = ops.reshape(output, (b, n, self.d))\n","        # (b, n, d) -> (b, n, d)\n","        output = self.out(output)\n","        \n","        return output\n","    \n","```\n"]},{"cell_type":"markdown","metadata":{"id":"lb0UqkGAliCu"},"source":["# Linear Transformer.\n","If a **LinearAttention** is followed by an **FFN**, a Linear Transformer is formed, where **FFN** is a mapping of the following form and  $\\sigma$ is activation function:\n","$$\n","\\mathrm{FFN}(\\mathbf X) = \\sigma(\\mathbf X W_1) \\mathbf W_2.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"M6uirw36qD1v"},"source":["## Implementation\n","In this part, we give the implementation of **FFN** and **LinearTransformer**, we also use ReLU as activation function $\\sigma$. The code is also available at `./python/needle/linear_transformer.py`:\n","\n","```python\n","class FFN(Module):\n","    def __init__(self, d, device=None, dtype=\"float32\"):\n","        super().__init__()\n","        self.module = Sequential(\n","            Linear3D(d, 2 * d, device=device, dtype=dtype),\n","            ReLU(),\n","            Linear3D(2 * d, d, device=device, dtype=dtype),\n","        )\n","\n","    def forward(self, x):\n","        return self.module(x)\n","    \n","class LinearTransformer(Module):\n","    def __init__(self, d, h, device=None, dtype=\"float32\"):\n","        super().__init__()\n","        self.module = Sequential(\n","            Residual(\n","                Sequential(\n","                    LayerNorm1d(d, device=device, dtype=dtype),\n","                    LinearAttention(d, h, device=device, dtype=dtype),\n","                )\n","            ),\n","            Residual(\n","                Sequential(\n","                    LayerNorm1d(d, device=device, dtype=dtype),\n","                    FFN(d, device=device, dtype=dtype),\n","                )\n","            ),\n","        )\n","        \n","    def forward(self, x):\n","        return self.module(x)\n","    \n","```"]},{"cell_type":"markdown","metadata":{"id":"c8RI6NCcyMvi"},"source":["## Linear Vit\n","Vit is a visual model proposed by Google. If you replace the **Transformer** with **LinearTransformer**, you can get `LinearVit`. Here we also made the following modifications to facilitate implementation:\n","- use convolution to implement patchfy;\n","- use mean pooling insead of cls token.\n","\n","We list the code below, which is also available at `./apps/linear_vit.py`:\n","```python\n","class LinearVit(ndl.nn.Module):\n","    def __init__(self, d=32, h=2, device=None, dtype=\"float32\"):\n","        super().__init__()\n","        self.patch_embedding = lt.PatchEmbedding(3, d, device=device, dtype=dtype)\n","        self.linear_transformer = nn.Sequential(\n","            lt.LinearTransformer(d, h, device=device, dtype=dtype),\n","        )\n","        self.mean = lt.Mean()\n","        self.linear = nn.Sequential(\n","            nn.Linear(d, 128, device=device, dtype=dtype), \n","            nn.ReLU(), \n","            nn.Linear(128, 10, device=device, dtype=dtype)\n","        )\n","        \n","    def forward(self, x):\n","        # b, d, h, w -> b, h1 * w1, d1\n","        x = self.patch_embedding(x)\n","        # b, h1 * w1, d1 -> b, h1 * w1, d1\n","        x = self.linear_transformer(x)\n","        # b, h1 * w1, d1 -> b, d1\n","        x = self.mean(x)\n","        # b, d1 -> b, m\n","        x = self.linear(x)\n","        \n","        return x\n","```"]},{"cell_type":"markdown","metadata":{"id":"9YtZ2cuXz5Uf"},"source":["# Train a model on Cifar-10 dataset\n","\n","Finally, let's train a classification model on CIFAR-10 dataset. The accuracy is about 41.5% after 10 epoch updates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxUqwa4UlgK_"},"outputs":[],"source":["import needle as ndl\n","import numpy as np\n","\n","from apps.linear_vit import LinearVit\n","\n","np.random.seed(0)\n","\n","# device = ndl.cpu()\n","device = ndl.cuda()\n","\n","def epoch_general(\n","    dataloader, model, loss_fn=ndl.nn.SoftmaxLoss(), opt=None, device=None\n","):\n","    if opt:\n","        model.train()\n","    else:\n","        model.eval()\n","    correct, total_loss = 0, 0\n","    cnt = 0\n","    for i, batch in enumerate(dataloader):\n","        if opt:\n","            opt.reset_grad()\n","        X, y = batch\n","        X, y = ndl.Tensor(X, device=device), ndl.Tensor(y, device=device)\n","        out = model(X)\n","        cnt += X.shape[0]\n","        correct += np.sum(np.argmax(out.numpy(), axis=1) == y.numpy())\n","        loss = loss_fn(out, y)\n","        total_loss += loss.data.numpy() * y.shape[0]\n","        if opt:\n","            loss.backward()\n","            opt.step()\n","        if opt and i % 10 == 0:\n","            print(\n","                f\"After update {i} times, the loss is {total_loss / cnt}, the accuracy is {correct / cnt}.\"\n","            )\n","\n","    return correct / cnt, total_loss / cnt\n","\n","\n","train_data = ndl.data.CIFAR10Dataset(\"./data/cifar-10-batches-py\", train=True)\n","test_data = ndl.data.CIFAR10Dataset(\"./data/cifar-10-batches-py\", train=False)\n","print(f\"number of train dataset: {train_data.n}\")\n","print(f\"number of test dataset: {test_data.n}\")\n","train_dataloader = ndl.data.DataLoader(\n","    dataset=train_data, batch_size=100, shuffle=False\n",")\n","test_dataloader = ndl.data.DataLoader(dataset=test_data, batch_size=100, shuffle=False)\n","model = LinearVit(device=device, dtype=\"float32\")\n","loss_fn = ndl.nn.SoftmaxLoss()\n","opt = ndl.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n","epochs = 10\n","for i in range(epochs):\n","    print(f\"Start epoch {i}\")\n","    train_acc, train_loss = epoch_general(\n","        dataloader=train_dataloader,\n","        model=model,\n","        loss_fn=loss_fn,\n","        opt=opt,\n","        device=device,\n","    )\n","\n","    test_acc, test_loss = epoch_general(\n","        dataloader=test_dataloader,\n","        model=model,\n","        loss_fn=loss_fn,\n","        opt=None,\n","        device=device,\n","    )\n","\n","    print(\n","        f\"After training {i} epochs, the loss is {test_loss}, the accuracy is {test_acc}.\"\n","    )"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPAgn7bOpeZ6ljkgHCJw9nZ","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.8 (default, Apr 13 2021, 19:58:26) \n[GCC 7.3.0]"},"vscode":{"interpreter":{"hash":"5a508d054a847b1ab5380d5f68f147db350197b3fd6c3c342950710fbfaf21f7"}}},"nbformat":4,"nbformat_minor":0}
